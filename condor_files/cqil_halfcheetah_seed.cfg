# Universe should always be 'vanilla' unless you know explicitly what you're doing
universe = vanilla

# this sets the environment variables of the login environment to the compute node environment
# generally, you will wcheetah to leave this set to true
getenv = true

env_id = HalfCheetahBulletEnv-v0
expert_episodes = 1



arguments = $(env_id) $(algo) $(seed) $(expert_episodes) 20 64 64 1e-5 100

#  $(expert_episodes) $(bc_epochs) $(bc_batch_size) $(bc_hidden_size) $(bc_ent_weight) $(bc_l2_weight)
executable = launch_files/cqil_train.sh

# this is the name and location of the Condor logs for your job. This will show details of what Condor itself is doing in relation to your job, but not the output of your job (for that, see the next entries)

log = /home/zli911/imitation/logs/$(env_id)/cqil_log_$(algo)_$(expert_episodes)_$(seed).log
output = /home/zli911/imitation/logs/$(env_id)/cqil_log_$(algo)_$(expert_episodes)_$(seed).out
error = /home/zli911/imitation/logs/$(env_id)/cqil_log_$(algo)_$(expert_episodes)_$(seed).error


notification = error
notification = complete
notify_user = zli911@gatech.edu

# If you need a specific amount of memory, Change this number to the minimum amount of RAM you need for your job. This number is in megabytes.

request_memory=4096

# If you need more than one CPU, Uncomment and change this number to the number of cores you need. The more CPUs you request the longer the job will take to queue.

request_gpus=0
# the queue command is REQUIRED
# if you need to run this more than once, enter the number of copies after the queue command
# for every copy spawned, the variable $(process) will increment by one, starting at zero. 
# if you only wish this to run once, leave the queue command with no number. 
# for example "queue 50" will spawn 50 copies of this job

# queue seed,expert_episodes,bc_epochs,bc_batch_size,bc_hidden_size,bc_ent_weight,bc_l2_weight from param_files/bc_params
queue algo, seed from (
    sac,0
    sac,1
    sac,2
    ppo,0
    ppo,1
    ppo,2
)